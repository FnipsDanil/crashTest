
services:
  backend:
    build:
      context: .
      dockerfile: ./backend/Dockerfile
    ports:
      - "8000:8000"
    restart: always
    container_name: api
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql+asyncpg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@pgbouncer:6432/${POSTGRES_DB}
      - TG_BOT_TOKEN=${TG_BOT_TOKEN}
      - WEB_APP_URL=${WEB_APP_URL}
      - PAYMENT_PROVIDER_TOKEN=${PAYMENT_PROVIDER_TOKEN}
      - WEBHOOK_SECRET=${WEBHOOK_SECRET}
      - WEBHOOK_BASE_URL=${WEBHOOK_BASE_URL:-http://localhost:8000}
      - CORS_ORIGINS=${CORS_ORIGINS}
      - TELEGRAM_ALERT_BOT_TOKEN=${TELEGRAM_ALERT_BOT_TOKEN}
      - ADMIN_CHAT_ID=${ADMIN_CHAT_ID}
      - CSRF_PROTECTED_ENDPOINTS=${CSRF_PROTECTED_ENDPOINTS}
      - GUNICORN_WORKERS=${GUNICORN_WORKERS:-4}
      # CRITICAL: Environment and debug settings
      - DEBUG=${DEBUG:-false}
      - ENVIRONMENT=${ENVIRONMENT:-production}
      # CRITICAL: Security keys
      - GAME_SECURITY_KEY=${GAME_SECURITY_KEY}
      # CSRF protection
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS}
      # PostgreSQL feature flags
      - DISABLE_POSTGRESQL_GAME_HISTORY=${DISABLE_POSTGRESQL_GAME_HISTORY:-false}
      - DISABLE_POSTGRESQL_BALANCE_UPDATES=${DISABLE_POSTGRESQL_BALANCE_UPDATES:-false}
      # TON/Star pricing
      - STAR_PRICE_USD=${STAR_PRICE_USD:-0.015}
      # Server configuration
      - SERVER_HOST=${SERVER_HOST:-0.0.0.0}
      - SERVER_PORT=${SERVER_PORT:-8000}
      - SUPPORT_USER_ID=${SUPPORT_USER_ID}
      # Channel subscription bonus feature
      - CHANNEL_BONUS_ENABLED=${CHANNEL_BONUS_ENABLED:-true}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    networks:
      - crash-stars-network
    command: >
      sh -c "gunicorn main:app \
      -k uvicorn.workers.UvicornWorker \
      --bind 0.0.0.0:8000 \
      --workers ${GUNICORN_WORKERS:-4} \
      --timeout 60 \
      --log-level warning"

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    # SECURITY: Removed external port exposure - PostgreSQL only accessible internally
    # ports:
    #   - "5432:5432"
    # Вариант 1 — 4 ядра, 8 ГБ RAM
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
      # - POSTGRES_SHARED_BUFFERS=2GB
      # - POSTGRES_EFFECTIVE_CACHE_SIZE=6GB
      # - POSTGRES_WORK_MEM=16MB
      # - POSTGRES_MAINTENANCE_WORK_MEM=256MB
      # - POSTGRES_MAX_CONNECTIONS=50
    # Вариант 2 — 8 ядер, 16 ГБ RAM
      # - POSTGRES_SHARED_BUFFERS=4GB               # 25% от RAM — оптимально
      # - POSTGRES_EFFECTIVE_CACHE_SIZE=12GB        # Оставшаяся RAM под кэш ОС и PgBouncer
      # - POSTGRES_WORK_MEM=32MB                     # Больше памяти для запросов
      # - POSTGRES_MAINTENANCE_WORK_MEM=512MB       # Для обслуживания базы
      # - POSTGRES_MAX_CONNECTIONS=100               # Чуть больше подключений, но без перегрузки
    # Вариант 3 — 12 ядер, 24 ГБ RAM
      # - POSTGRES_SHARED_BUFFERS=6GB               # 25% от RAM — оптимально
      # - POSTGRES_EFFECTIVE_CACHE_SIZE=18GB        # Оставшаяся RAM под кэш ОС и PgBouncer
      # - POSTGRES_WORK_MEM=48MB                     # Больше памяти для сложных запросов
      # - POSTGRES_MAINTENANCE_WORK_MEM=1GB         # Увеличиваем для обслуживания базы
      # - POSTGRES_MAX_CONNECTIONS=150               # Больше подключений с учетом мощности
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database_schema.sql:/docker-entrypoint-initdb.d/01-schema.sql
    networks:
      - crash-stars-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 5s
      timeout: 5s
      retries: 5

  pgbouncer:
    image: edoburu/pgbouncer:latest
    container_name: pgbouncer
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    # Вариант 1 — 4 ядра, 8 ГБ RAM
    environment:
      # - PGBOUNCER_POOL_MODE=transaction
      # - PGBOUNCER_MAX_CLIENT_CONN=100              # Максимум клиентов
      # - PGBOUNCER_DEFAULT_POOL_SIZE=20             # Кол-во одновременных соединений к Postgres, примерно 5 на ядро
      # - PGBOUNCER_MIN_POOL_SIZE=5
      # - PGBOUNCER_RESERVE_POOL_SIZE=5
      # - PGBOUNCER_LOG_CONNECTIONS=1
      # - PGBOUNCER_LOG_DISCONNECTIONS=1
      # - PGBOUNCER_LOG_POOLER_ERRORS=1
      # - PGBOUNCER_ADMIN_USERS=${POSTGRES_USER}
      # - PGBOUNCER_AUTH_TYPE=trust                   # Безопасность лучше настраивать в userlist.txt
    # Вариант 2 — 8 ядер, 16 ГБ RAM
      # - PGBOUNCER_POOL_MODE=transaction
      # - PGBOUNCER_MAX_CLIENT_CONN=200               # Больше клиентов с учетом нагрузки
      # - PGBOUNCER_DEFAULT_POOL_SIZE=40              # ~5 соединений на ядро
      # - PGBOUNCER_MIN_POOL_SIZE=10
      # - PGBOUNCER_RESERVE_POOL_SIZE=10
      # - PGBOUNCER_LOG_CONNECTIONS=1
      # - PGBOUNCER_LOG_DISCONNECTIONS=1
      # - PGBOUNCER_LOG_POOLER_ERRORS=1
      # - PGBOUNCER_ADMIN_USERS=${POSTGRES_USER}
      # - PGBOUNCER_AUTH_TYPE=trust
    # Вариант 3 — 12 ядер, 24 ГБ RAM
      - PGBOUNCER_POOL_MODE=transaction
      #- PGBOUNCER_MAX_CLIENT_CONN=300               # Ещё больше клиентов с учетом мощности
      #- PGBOUNCER_DEFAULT_POOL_SIZE=60              # ~5 соединений на ядро
      #- PGBOUNCER_MIN_POOL_SIZE=15
      #- PGBOUNCER_RESERVE_POOL_SIZE=15
      - PGBOUNCER_LOG_CONNECTIONS=1
      - PGBOUNCER_LOG_DISCONNECTIONS=1
      - PGBOUNCER_LOG_POOLER_ERRORS=1
      - PGBOUNCER_ADMIN_USERS=${POSTGRES_USER}
      - PGBOUNCER_AUTH_TYPE=trust
    volumes:
      - ./pgbouncer.ini:/etc/pgbouncer/pgbouncer.ini:ro
      - ./userlist.txt:/etc/pgbouncer/userlist.txt:ro
    ports:
      - "127.0.0.1:6432:6432"  # PgBouncer только для localhost (SSH туннель)
    depends_on:
      - postgres
    networks:
      - crash-stars-network

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"
    volumes:
      - redis_data:/data
    networks:
      - crash-stars-network
    # Вариант 1 — 4 ядра, 8 ГБ RAM
    # command: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru
    # Вариант 2 — 8 ядер, 16 ГБ RAM
    # command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru
    # Вариант 3 — 12 ядер, 24 ГБ RAM
    command: redis-server --maxmemory 2gb --maxmemory-policy allkeys-lru

  # telegram-bot:
  #   build: ./telegram-bot
  #   container_name: telegram-bot
  #   environment:
  #     - TG_BOT_TOKEN=${TG_BOT_TOKEN}
  #     - WEB_APP_URL=${WEB_APP_URL:-https://localhost:5173/}
  #     - SUPPORT_USER_ID=${SUPPORT_USER_ID}
  #     - BACKEND_URL=${BACKEND_URL:-http://backend:8000}
  #   networks:
  #     - crash-stars-network
  #   restart: unless-stopped

  frontend:
    build: 
      context: ./frontend
      args:
        - VITE_API_URL=${VITE_API_URL:-https://que-crash.fun}
        - VITE_HMAC_SECRET_KEY=${VITE_HMAC_SECRET_KEY}
        - VITE_GIFT_BOT_USERNAME=${VITE_GIFT_BOT_USERNAME:-CrashStars}
    container_name: frontend
    networks:
      - crash-stars-network
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # # Health exporter для мониторинга контейнеров
  # health-exporter:
  #   build: ./health-exporter
  #   container_name: health-exporter
  #   restart: always
  #   networks:
  #     - crash-stars-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 32M
  #       reservations:
  #         memory: 16M
  #   # Добавляем health check для самого экспортера
  #   healthcheck:
  #     test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 3
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"

  # # Легковесный мониторинг системы
  # node-exporter:
  #   image: prom/node-exporter:latest
  #   container_name: node-exporter
  #   restart: always
  #   command:
  #     - '--path.rootfs=/rootfs'
  #     - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
  #     - '--collector.disable-defaults'
  #     - '--collector.cpu'
  #     - '--collector.meminfo'
  #     - '--collector.diskstats'
  #     - '--collector.filesystem'
  #     - '--collector.loadavg'
  #     - '--collector.netstat'
  #   volumes:
  #     - /proc:/host/proc:ro
  #     - /sys:/host/sys:ro
  #     - /:/rootfs:ro
  #   networks:
  #     - crash-stars-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 32M
  #       reservations:
  #         memory: 16M
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"

  # # Легковесный Prometheus для сбора метрик
  # prometheus:
  #   image: prom/prometheus:latest
  #   container_name: prometheus
  #   restart: always
  #   command:
  #     - '--config.file=/etc/prometheus/prometheus.yml'
  #     - '--storage.tsdb.path=/prometheus'
  #     - '--storage.tsdb.retention.time=7d'  # Храним только 7 дней
  #     - '--storage.tsdb.retention.size=500MB'  # Максимум 500MB
  #     - '--web.console.libraries=/etc/prometheus/console_libraries'
  #     - '--web.console.templates=/etc/prometheus/consoles'
  #     - '--web.enable-lifecycle'
  #     - '--web.route-prefix=/'
  #     - '--web.external-url=https://${GRAFANA_DOMAIN:-localhost}/prometheus/'
  #   volumes:
  #     - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
  #     - prometheus_data:/prometheus
  #   depends_on:
  #     - node-exporter
  #     - health-exporter
  #   networks:
  #     - crash-stars-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 128M
  #       reservations:
  #         memory: 64M
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"

  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: grafana
  #   restart: always
  #   environment:
  #     # Безопасность: отключаем анонимный доступ
  #     - GF_AUTH_ANONYMOUS_ENABLED=false
  #     - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
  #     - GF_AUTH_DISABLE_LOGIN_FORM=false
  #     - GF_AUTH_DISABLE_SIGNOUT_MENU=false
  #     # Безопасность: настройки администратора
  #     - GF_SECURITY_ADMIN_USER=admin
  #     - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
  #     - GF_SECURITY_SECRET_KEY=${GRAFANA_SECRET_KEY}
  #     # Безопасность: HTTPS и cookie
  #     - GF_SERVER_PROTOCOL=http
  #     - GF_SERVER_DOMAIN=grafana.que-crash.fun
  #     - GF_SERVER_ROOT_URL=https://grafana.que-crash.fun/
  #     - GF_SERVER_SERVE_FROM_SUB_PATH=false
  #     # Ограничения безопасности
  #     - GF_SECURITY_ALLOW_EMBEDDING=false
  #     - GF_SECURITY_COOKIE_SECURE=true
  #     - GF_SECURITY_COOKIE_SAMESITE=strict
  #     - GF_SECURITY_STRICT_TRANSPORT_SECURITY=true
  #     # Отключаем регистрацию новых пользователей
  #     - GF_USERS_ALLOW_SIGN_UP=false
  #     - GF_USERS_ALLOW_ORG_CREATE=false
  #     # Логирование для безопасности
  #     - GF_LOG_LEVEL=info
  #     - GF_LOG_MODE=console
  #     # Оптимизация ресурсов
  #     - GF_ANALYTICS_REPORTING_ENABLED=false
  #     - GF_ANALYTICS_CHECK_FOR_UPDATES=false
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #     - ./grafana/provisioning:/etc/grafana/provisioning
  #   depends_on:
  #     - postgres
  #     - prometheus
  #   networks:
  #     - crash-stars-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 256M
  #       reservations:
  #         memory: 128M
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "10m"
  #       max-file: "3"
  #   # Не экспонируем порт наружу - доступ только через nginx
  #   # ports:
  #   #   - "3000:3000"

  # # Loki для централизованного хранения логов
  # loki:
  #   image: grafana/loki:latest
  #   container_name: loki
  #   restart: always
  #   command:
  #     - -config.file=/etc/loki/loki-config.yml
  #     - -target=all
  #   volumes:
  #     - ./loki-config.yml:/etc/loki/loki-config.yml:ro
  #     - loki_data:/loki
  #   networks:
  #     - crash-stars-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 256M
  #       reservations:
  #         memory: 128M
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "5m"
  #       max-file: "2"

  # # Promtail для сбора логов всех сервисов
  # promtail:
  #   image: grafana/promtail:latest
  #   container_name: promtail
  #   restart: always
  #   volumes:
  #     - ./promtail-config.yml:/etc/promtail/config.yml:ro
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #     - /var/log:/var/log:ro
  #     - /var/lib/docker/containers:/var/lib/docker/containers:ro
  #   command: -config.file=/etc/promtail/config.yml
  #   depends_on:
  #     - loki
  #   networks:
  #     - crash-stars-network
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 64M
  #       reservations:
  #         memory: 32M
  #   logging:
  #     driver: "json-file"
  #     options:
  #       max-size: "5m"
  #       max-file: "2"

  nginx:
    image: nginx:latest
    restart: always
    container_name: nginx
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "5"
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - backend
      - frontend
      #- grafana
    networks:
      - crash-stars-network
    volumes:
      - ./nginx-dev.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      #- ./.htpasswd:/etc/nginx/.htpasswd # need to generate by ```sudo htpasswd -c /etc/nginx/.htpasswd grafana_user```

volumes:
  redis_data:
  postgres_data:
  #grafana_data:
  #prometheus_data:
  #loki_data:

networks:
  crash-stars-network:
    driver: bridge
